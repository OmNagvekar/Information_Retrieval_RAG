from langchain_huggingface import HuggingFaceEndpoint,HuggingFacePipeline,ChatHuggingFace
from langchain_core.prompts import ChatPromptTemplate
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline

model_id = "NousResearch/Hermes-3-Llama-3.2-3B-GGUF"
filename = "Hermes-3-Llama-3.2-3B.Q4_K_M.gguf"

tokenizer = AutoTokenizer.from_pretrained(model_id, gguf_file=filename)
model = AutoModelForCausalLM.from_pretrained(model_id, gguf_file=filename)
pipe = pipeline(
    "text-generation", model=model, tokenizer=tokenizer,device='cpu',temperature=0.5
)

llm = HuggingFacePipeline(pipeline=pipe)
chat_model = ChatHuggingFace(llm=llm)

prompt_template = ChatPromptTemplate.from_messages([
    ("system","Translate the following English text to French:"), 
    ("human","text:{text}")
])

# Create an LLMChain with the LLM and the prompt template
llm_chain = prompt_template | chat_model

# Define the input text
input_text = "Hello, how are you?"

# Run the LLMChain with the input text
output = llm_chain.invoke({"text": input_text})

# Print the output
print(output.content)